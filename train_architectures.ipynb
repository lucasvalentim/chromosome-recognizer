{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ZleVVSQig9CF",
    "outputId": "1794a561-8e98-4a20-afc4-54f2d8804e12"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalMaxPooling2D, BatchNormalization\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IC0KxROjhMsR"
   },
   "outputs": [],
   "source": [
    "def load_images(dirname, filetype=None, grayscale=False, size=None):\n",
    "    mode = 0 if grayscale else 1\n",
    "    images = []\n",
    "    filenames = os.listdir(dirname)\n",
    "    \n",
    "    if filetype is not None:\n",
    "        filenames = [filename for filename in filenames if filename.endswith('.' + filetype)]\n",
    "        \n",
    "    for filename in filenames:\n",
    "        image = cv2.imread(os.path.join(dirname, filename), mode)\n",
    "        \n",
    "        if size is not None:\n",
    "            image = cv2.resize(image, size)\n",
    "        \n",
    "        images.append(image)\n",
    "    \n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8W7Qb0JIhzI-"
   },
   "outputs": [],
   "source": [
    "def prepare_data(c, nc, expand_dims=False, normalization_method=None, return_y=False, y_categorical=False):\n",
    "    c  = np.array(c) if type(c) == list else c\n",
    "    nc = np.array(nc) if type(nc) == list else nc\n",
    "    \n",
    "    X = np.concatenate((c, nc), axis=0)\n",
    "    \n",
    "    if expand_dims:\n",
    "        X = np.expand_dims(X, axis=3)\n",
    "    \n",
    "    if normalization_method is not None:\n",
    "        X = X.astype(np.float32)\n",
    "        \n",
    "        if normalization_method == 1:\n",
    "            X = (X - X.min()) / (X.max() - X.min())\n",
    "        \n",
    "        elif normalization_method == 2:\n",
    "            X = 2 * (X - X.min()) / (X.max() - X.min())\n",
    "            \n",
    "        elif normalization_method == 3:\n",
    "            X = (X - X.mean()) / X.std()\n",
    "    \n",
    "    if return_y:\n",
    "        c_label  = np.ones(c.shape[0])\n",
    "        nc_label = np.zeros(nc.shape[0])\n",
    "        \n",
    "        y = np.concatenate((c_label, nc_label), axis=0)\n",
    "        \n",
    "        if y_categorical:\n",
    "            y = to_categorical(y)\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ArjmrH-El0M4"
   },
   "outputs": [],
   "source": [
    "def train_multiple_times(model_builder, times, weights_path_format, **kwargs):\n",
    "    weights_dirname = '/'.join(weights_path_format.split('/')[:-1])\n",
    "    \n",
    "    if not os.path.exists(weights_dirname):\n",
    "        os.makedirs(weights_dirname)\n",
    "    \n",
    "    for t in range(times):\n",
    "        weights_path = weights_path_format.format(t + 1)\n",
    "        \n",
    "        print('Treinamento {} de {}, os pesos serão salvos em {}'.format(t + 1, times, weights_path))\n",
    "        \n",
    "        checkpointer = ModelCheckpoint(weights_path, save_best_only=True, verbose=0)\n",
    "        \n",
    "        model = model_builder()\n",
    "        \n",
    "        model.fit(callbacks=[checkpointer], verbose=0, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nwxmOPhtHoIT"
   },
   "source": [
    "## Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WYi0nC6Yhq5r"
   },
   "source": [
    "### Modelo 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "09AhI5dlk0xt"
   },
   "outputs": [],
   "source": [
    "def model_1():\n",
    "    net_input = Input(shape=(128, 128, 1))\n",
    "    \n",
    "    net_layer = Conv2D(32, (3, 3), activation='relu')(net_input)\n",
    "    net_layer = MaxPooling2D((2, 2))(net_layer)\n",
    "    net_layer = Dropout(0.25)(net_layer)\n",
    "    \n",
    "    net_layer = Conv2D(128, (1,1))(net_layer)\n",
    "    net_layer = GlobalMaxPooling2D()(net_layer)\n",
    "    \n",
    "    net_output = Dense(1, activation='sigmoid')(net_layer)\n",
    "    \n",
    "    model = Model(inputs=net_input, outputs=net_output)\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a4-g5q-why2-"
   },
   "source": [
    "### Modelo 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sDImktt4XC3u"
   },
   "outputs": [],
   "source": [
    "def model_2():\n",
    "    net_input = Input(shape=(128, 128, 3))\n",
    "    \n",
    "    net_layer = Conv2D(32, (3, 3), activation='relu')(net_input)\n",
    "    net_layer = MaxPooling2D((2, 2))(net_layer)\n",
    "    net_layer = Dropout(0.25)(net_layer)\n",
    "    \n",
    "    net_layer = Conv2D(128, (1,1))(net_layer)\n",
    "    net_layer = GlobalMaxPooling2D()(net_layer)\n",
    "    \n",
    "    net_output = Dense(1, activation='sigmoid')(net_layer)\n",
    "    \n",
    "    model = Model(inputs=net_input, outputs=net_output)\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kbPSAAr8kVzL"
   },
   "source": [
    "### Modelo 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MtE2yNdvkVzN"
   },
   "outputs": [],
   "source": [
    "def model_3():\n",
    "    net_input = Input(shape=(128, 128, 3))\n",
    "    \n",
    "    net_layer = Conv2D(32, (3, 3), activation='relu')(net_input)\n",
    "    net_layer = MaxPooling2D((2, 2))(net_layer)\n",
    "    net_layer = Dropout(0.25)(net_layer)\n",
    "    \n",
    "    net_layer = Conv2D(128, (1,1))(net_layer)\n",
    "    net_layer = GlobalMaxPooling2D()(net_layer)\n",
    "    \n",
    "    net_output = Dense(2, activation='softmax')(net_layer)\n",
    "    \n",
    "    model = Model(inputs=net_input, outputs=net_output)\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i9OfxVj3Nl-9"
   },
   "source": [
    "### Modelo 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F4mISeWxNl_B"
   },
   "outputs": [],
   "source": [
    "def model_4():\n",
    "    net_input = Input(shape=(128, 128, 3))\n",
    "    \n",
    "    net_layer = Conv2D(32, (3, 3), activation='relu')(net_input)\n",
    "    net_layer = BatchNormalization()(net_layer)\n",
    "    net_layer = MaxPooling2D((2, 2))(net_layer)\n",
    "    net_layer = Dropout(0.25)(net_layer)\n",
    "    \n",
    "    net_layer = Conv2D(128, (1,1))(net_layer)\n",
    "    net_layer = GlobalMaxPooling2D()(net_layer)\n",
    "    \n",
    "    net_output = Dense(1, activation='sigmoid')(net_layer)\n",
    "    \n",
    "    model = Model(inputs=net_input, outputs=net_output)\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OuVxz4GwPFQS"
   },
   "source": [
    "### Modelo 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AreM8HSDPFQU"
   },
   "outputs": [],
   "source": [
    "def model_5():\n",
    "    net_input = Input(shape=(128, 128, 3))\n",
    "    \n",
    "    net_layer = Conv2D(32, (3, 3), activation='relu')(net_input)\n",
    "    net_layer = MaxPooling2D((2, 2))(net_layer)\n",
    "    net_layer = BatchNormalization()(net_layer)\n",
    "    net_layer = Dropout(0.25)(net_layer)\n",
    "    \n",
    "    net_layer = Conv2D(128, (1,1))(net_layer)\n",
    "    net_layer = GlobalMaxPooling2D()(net_layer)\n",
    "    \n",
    "    net_output = Dense(1, activation='sigmoid')(net_layer)\n",
    "    \n",
    "    model = Model(inputs=net_input, outputs=net_output)\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tQ4WCqfQpq2V"
   },
   "source": [
    "### Modelo 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pkCFClbupq2Y"
   },
   "outputs": [],
   "source": [
    "def model_6():\n",
    "    net_input = Input(shape=(128, 128, 3))\n",
    "    \n",
    "    net_layer = Conv2D(32, (3, 3), activation='relu')(net_input)\n",
    "    net_layer = MaxPooling2D((2, 2))(net_layer)\n",
    "    net_layer = BatchNormalization()(net_layer)\n",
    "    net_layer = Dropout(0.25)(net_layer)\n",
    "    \n",
    "    net_layer = Conv2D(128, (1,1))(net_layer)\n",
    "    net_layer = GlobalMaxPooling2D()(net_layer)\n",
    "    \n",
    "    net_layer = Dense(128, activation='relu')(net_layer)\n",
    "    net_layer = Dropout(0.5)(net_layer)\n",
    "    \n",
    "    net_output = Dense(1, activation='sigmoid')(net_layer)\n",
    "    \n",
    "    model = Model(inputs=net_input, outputs=net_output)\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MCI82tQSrYOJ"
   },
   "source": [
    "### Modelo 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wu3flDVHrYOL"
   },
   "outputs": [],
   "source": [
    "def model_7():\n",
    "    net_input = Input(shape=(128, 128, 3))\n",
    "    \n",
    "    net_layer = Conv2D(32, (3, 3), activation='relu')(net_input)\n",
    "    net_layer = MaxPooling2D((2, 2))(net_layer)\n",
    "    net_layer = BatchNormalization()(net_layer)\n",
    "    net_layer = Dropout(0.25)(net_layer)\n",
    "    \n",
    "    net_layer = Conv2D(128, (1,1))(net_layer)\n",
    "    net_layer = GlobalMaxPooling2D()(net_layer)\n",
    "    \n",
    "    net_layer = Dense(256, activation='relu')(net_layer)\n",
    "    net_layer = Dropout(0.5)(net_layer)\n",
    "    \n",
    "    net_output = Dense(1, activation='sigmoid')(net_layer)\n",
    "    \n",
    "    model = Model(inputs=net_input, outputs=net_output)\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ccD_PvmdxaLx"
   },
   "source": [
    "### Modelo 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hi678VAnxaL2"
   },
   "outputs": [],
   "source": [
    "def model_8():\n",
    "    net_input = Input(shape=(128, 128, 3))\n",
    "    \n",
    "    net_layer = Conv2D(32, (3, 3), activation='relu')(net_input)\n",
    "    net_layer = MaxPooling2D((2, 2))(net_layer)\n",
    "    net_layer = BatchNormalization()(net_layer)\n",
    "    net_layer = Dropout(0.25)(net_layer)\n",
    "    \n",
    "    net_layer = Conv2D(64, (3, 3), activation='relu')(net_layer)\n",
    "    net_layer = MaxPooling2D((2, 2))(net_layer)\n",
    "    net_layer = BatchNormalization()(net_layer)\n",
    "    net_layer = Dropout(0.25)(net_layer)\n",
    "    \n",
    "    net_layer = Conv2D(128, (1,1))(net_layer)\n",
    "    net_layer = GlobalMaxPooling2D()(net_layer)\n",
    "    \n",
    "    net_layer = Dense(256, activation='relu')(net_layer)\n",
    "    net_layer = Dropout(0.5)(net_layer)\n",
    "    \n",
    "    net_output = Dense(1, activation='sigmoid')(net_layer)\n",
    "    \n",
    "    model = Model(inputs=net_input, outputs=net_output)\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NQxPLo6JAp-P"
   },
   "source": [
    "### Modelo 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3IJGD5lGAp-S"
   },
   "outputs": [],
   "source": [
    "def model_9():\n",
    "    net_input = Input(shape=(128, 128, 3))\n",
    "    \n",
    "    net_layer = Conv2D(32, (3, 3), activation='relu')(net_input)\n",
    "    net_layer = MaxPooling2D((2, 2))(net_layer)\n",
    "    net_layer = BatchNormalization()(net_layer)\n",
    "    net_layer = Dropout(0.25)(net_layer)\n",
    "    \n",
    "    net_layer = Conv2D(64, (5, 5), activation='relu')(net_layer)\n",
    "    net_layer = MaxPooling2D((2, 2))(net_layer)\n",
    "    net_layer = BatchNormalization()(net_layer)\n",
    "    net_layer = Dropout(0.25)(net_layer)\n",
    "    \n",
    "    net_layer = Conv2D(128, (1,1))(net_layer)\n",
    "    net_layer = GlobalMaxPooling2D()(net_layer)\n",
    "    \n",
    "    net_layer = Dense(256, activation='relu')(net_layer)\n",
    "    net_layer = Dropout(0.5)(net_layer)\n",
    "    \n",
    "    net_output = Dense(1, activation='sigmoid')(net_layer)\n",
    "    \n",
    "    model = Model(inputs=net_input, outputs=net_output)\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fpJ74jzLSv5t"
   },
   "source": [
    "### Modelo 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0vxx0CPOSv5x"
   },
   "outputs": [],
   "source": [
    "def model_10():\n",
    "    net_input = Input(shape=(128, 128, 3))\n",
    "    \n",
    "    net_layer = Conv2D(32, (3, 3), activation='relu')(net_input)\n",
    "    net_layer = MaxPooling2D((2, 2))(net_layer)\n",
    "    net_layer = BatchNormalization()(net_layer)\n",
    "    net_layer = Dropout(0.25)(net_layer)\n",
    "    \n",
    "    net_layer = Conv2D(64, (3, 3), activation='relu')(net_layer)\n",
    "    net_layer = MaxPooling2D((2, 2))(net_layer)\n",
    "    net_layer = BatchNormalization()(net_layer)\n",
    "    net_layer = Dropout(0.25)(net_layer)\n",
    "    \n",
    "    net_layer = Conv2D(128, (3, 3), activation='relu')(net_layer)\n",
    "    net_layer = MaxPooling2D((2, 2))(net_layer)\n",
    "    net_layer = BatchNormalization()(net_layer)\n",
    "    net_layer = Dropout(0.25)(net_layer)\n",
    "    \n",
    "    net_layer = Conv2D(128, (1,1))(net_layer)\n",
    "    net_layer = GlobalMaxPooling2D()(net_layer)\n",
    "    \n",
    "    net_layer = Dense(256, activation='relu')(net_layer)\n",
    "    net_layer = Dropout(0.5)(net_layer)\n",
    "    \n",
    "    net_output = Dense(1, activation='sigmoid')(net_layer)\n",
    "    \n",
    "    model = Model(inputs=net_input, outputs=net_output)\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ObU0MXy7qOxM"
   },
   "source": [
    "### Modelo 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hh_m-2nkqOxP"
   },
   "outputs": [],
   "source": [
    "def model_11():\n",
    "    net_input = Input(shape=(128, 128, 3))\n",
    "    \n",
    "    net_layer = Conv2D(32, (3, 3), activation='relu')(net_input)\n",
    "    net_layer = MaxPooling2D((2, 2))(net_layer)\n",
    "    net_layer = BatchNormalization()(net_layer)\n",
    "    net_layer = Dropout(0.25)(net_layer)\n",
    "    \n",
    "    net_layer = Conv2D(64, (5, 5), activation='relu')(net_layer)\n",
    "    net_layer = MaxPooling2D((2, 2))(net_layer)\n",
    "    net_layer = BatchNormalization()(net_layer)\n",
    "    net_layer = Dropout(0.25)(net_layer)\n",
    "    \n",
    "    net_layer = Conv2D(128, (7, 7), activation='relu')(net_layer)\n",
    "    net_layer = MaxPooling2D((2, 2))(net_layer)\n",
    "    net_layer = BatchNormalization()(net_layer)\n",
    "    net_layer = Dropout(0.25)(net_layer)\n",
    "    \n",
    "    net_layer = Conv2D(128, (1,1))(net_layer)\n",
    "    net_layer = GlobalMaxPooling2D()(net_layer)\n",
    "    \n",
    "    net_layer = Dense(256, activation='relu')(net_layer)\n",
    "    net_layer = Dropout(0.5)(net_layer)\n",
    "    \n",
    "    net_output = Dense(1, activation='sigmoid')(net_layer)\n",
    "    \n",
    "    model = Model(inputs=net_input, outputs=net_output)\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OWDtTZM2Hy9A"
   },
   "source": [
    "## Treinamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IBQVfLGEEinI"
   },
   "source": [
    "### Arquitetura 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yGBguevV_QuW"
   },
   "outputs": [],
   "source": [
    "c_train  = load_images('Dataset_2/Train/C', filetype='tif', grayscale=True, size=(128, 128))\n",
    "nc_train = load_images('Dataset_2/Train/NC', filetype='tif', grayscale=True, size=(128, 128))\n",
    "\n",
    "c_validation  = load_images('Dataset_2/Validation/C', filetype='tif', grayscale=True, size=(128, 128))\n",
    "nc_validation = load_images('Dataset_2/Validation/NC', filetype='tif', grayscale=True, size=(128, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FBoqYeMd_rXY"
   },
   "outputs": [],
   "source": [
    "X_train, y_train = prepare_data(c_train, nc_train, expand_dims=True, normalization_method=1, return_y=True)\n",
    "X_validation, y_validation = prepare_data(c_validation, nc_validation, expand_dims=True, normalization_method=1, return_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "Oay0ivZdAjBf",
    "outputId": "13c75877-6c5a-41f5-8fc3-fb454af58f97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treinamento 1 de 10, os pesos serão salvos em weights/architecture_1/training_1.hdf5\n",
      "Treinamento 2 de 10, os pesos serão salvos em weights/architecture_1/training_2.hdf5\n",
      "Treinamento 3 de 10, os pesos serão salvos em weights/architecture_1/training_3.hdf5\n",
      "Treinamento 4 de 10, os pesos serão salvos em weights/architecture_1/training_4.hdf5\n",
      "Treinamento 5 de 10, os pesos serão salvos em weights/architecture_1/training_5.hdf5\n",
      "Treinamento 6 de 10, os pesos serão salvos em weights/architecture_1/training_6.hdf5\n",
      "Treinamento 7 de 10, os pesos serão salvos em weights/architecture_1/training_7.hdf5\n",
      "Treinamento 8 de 10, os pesos serão salvos em weights/architecture_1/training_8.hdf5\n",
      "Treinamento 9 de 10, os pesos serão salvos em weights/architecture_1/training_9.hdf5\n",
      "Treinamento 10 de 10, os pesos serão salvos em weights/architecture_1/training_10.hdf5\n"
     ]
    }
   ],
   "source": [
    "train_multiple_times(model_builder=model_1, times=10, weights_path_format='weights/architecture_1/training_{}.hdf5',\n",
    "                     x=X_train, y=y_train, batch_size=32, epochs=50, validation_data=(X_validation, y_validation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ej0N5B86VMHF"
   },
   "source": [
    "### Arquitetura 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8nnYO9sacBWC"
   },
   "outputs": [],
   "source": [
    "c_train  = load_images('Dataset_2/Train/C', filetype='tif', size=(128, 128))\n",
    "nc_train = load_images('Dataset_2/Train/NC', filetype='tif', size=(128, 128))\n",
    "\n",
    "c_validation  = load_images('Dataset_2/Validation/C', filetype='tif', size=(128, 128))\n",
    "nc_validation = load_images('Dataset_2/Validation/NC', filetype='tif', size=(128, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q-IIwYegcBWH"
   },
   "outputs": [],
   "source": [
    "X_train, y_train = prepare_data(c_train, nc_train, normalization_method=1, return_y=True)\n",
    "X_validation, y_validation = prepare_data(c_validation, nc_validation, normalization_method=1, return_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "WnMsgN-ycBWL",
    "outputId": "9760e4ad-85e2-4add-bdf0-75c773b4036c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treinamento 1 de 10, os pesos serão salvos em weights/architecture_2/training_1.hdf5\n",
      "Treinamento 2 de 10, os pesos serão salvos em weights/architecture_2/training_2.hdf5\n",
      "Treinamento 3 de 10, os pesos serão salvos em weights/architecture_2/training_3.hdf5\n",
      "Treinamento 4 de 10, os pesos serão salvos em weights/architecture_2/training_4.hdf5\n",
      "Treinamento 5 de 10, os pesos serão salvos em weights/architecture_2/training_5.hdf5\n",
      "Treinamento 6 de 10, os pesos serão salvos em weights/architecture_2/training_6.hdf5\n",
      "Treinamento 7 de 10, os pesos serão salvos em weights/architecture_2/training_7.hdf5\n",
      "Treinamento 8 de 10, os pesos serão salvos em weights/architecture_2/training_8.hdf5\n",
      "Treinamento 9 de 10, os pesos serão salvos em weights/architecture_2/training_9.hdf5\n",
      "Treinamento 10 de 10, os pesos serão salvos em weights/architecture_2/training_10.hdf5\n"
     ]
    }
   ],
   "source": [
    "train_multiple_times(model_builder=model_2, times=10, weights_path_format='weights/architecture_2/training_{}.hdf5',\n",
    "                     x=X_train, y=y_train, batch_size=32, epochs=50, validation_data=(X_validation, y_validation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kn9M7wBDleL1"
   },
   "source": [
    "### Arquitetura 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r1dNzwV-leL6"
   },
   "outputs": [],
   "source": [
    "c_train  = load_images('Dataset_2/Train/C', filetype='tif', size=(128, 128))\n",
    "nc_train = load_images('Dataset_2/Train/NC', filetype='tif', size=(128, 128))\n",
    "\n",
    "c_validation  = load_images('Dataset_2/Validation/C', filetype='tif', size=(128, 128))\n",
    "nc_validation = load_images('Dataset_2/Validation/NC', filetype='tif', size=(128, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dtkRWZveleL-"
   },
   "outputs": [],
   "source": [
    "X_train, y_train = prepare_data(c_train, nc_train, normalization_method=1, return_y=True, y_categorical=True)\n",
    "X_validation, y_validation = prepare_data(c_validation, nc_validation, normalization_method=1, return_y=True, y_categorical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "Lvxo3gmXleME",
    "outputId": "5dd8208d-1968-4785-e196-3dc1d896f4a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treinamento 1 de 10, os pesos serão salvos em weights/architecture_3/training_1.hdf5\n",
      "Treinamento 2 de 10, os pesos serão salvos em weights/architecture_3/training_2.hdf5\n",
      "Treinamento 3 de 10, os pesos serão salvos em weights/architecture_3/training_3.hdf5\n",
      "Treinamento 4 de 10, os pesos serão salvos em weights/architecture_3/training_4.hdf5\n",
      "Treinamento 5 de 10, os pesos serão salvos em weights/architecture_3/training_5.hdf5\n",
      "Treinamento 6 de 10, os pesos serão salvos em weights/architecture_3/training_6.hdf5\n",
      "Treinamento 7 de 10, os pesos serão salvos em weights/architecture_3/training_7.hdf5\n",
      "Treinamento 8 de 10, os pesos serão salvos em weights/architecture_3/training_8.hdf5\n",
      "Treinamento 9 de 10, os pesos serão salvos em weights/architecture_3/training_9.hdf5\n",
      "Treinamento 10 de 10, os pesos serão salvos em weights/architecture_3/training_10.hdf5\n"
     ]
    }
   ],
   "source": [
    "train_multiple_times(model_builder=model_3, times=10, weights_path_format='weights/architecture_3/training_{}.hdf5',\n",
    "                     x=X_train, y=y_train, batch_size=32, epochs=50, validation_data=(X_validation, y_validation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0rp9x0Q7yjQ0"
   },
   "source": [
    "### Arquitetura 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tukBe2XhyjQ8"
   },
   "outputs": [],
   "source": [
    "c_train  = load_images('Dataset_2/Train/C', filetype='tif', size=(128, 128))\n",
    "nc_train = load_images('Dataset_2/Train/NC', filetype='tif', size=(128, 128))\n",
    "\n",
    "c_validation  = load_images('Dataset_2/Validation/C', filetype='tif', size=(128, 128))\n",
    "nc_validation = load_images('Dataset_2/Validation/NC', filetype='tif', size=(128, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aI-X6XbHyjRB"
   },
   "outputs": [],
   "source": [
    "X_train, y_train = prepare_data(c_train, nc_train, normalization_method=2, return_y=True)\n",
    "X_validation, y_validation = prepare_data(c_validation, nc_validation, normalization_method=2, return_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "o4weNgJDyjRI",
    "outputId": "e39227ab-04ca-48af-82bf-fa30ad5150f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treinamento 1 de 10, os pesos serão salvos em weights/architecture_4/training_1.hdf5\n",
      "Treinamento 2 de 10, os pesos serão salvos em weights/architecture_4/training_2.hdf5\n",
      "Treinamento 3 de 10, os pesos serão salvos em weights/architecture_4/training_3.hdf5\n",
      "Treinamento 4 de 10, os pesos serão salvos em weights/architecture_4/training_4.hdf5\n",
      "Treinamento 5 de 10, os pesos serão salvos em weights/architecture_4/training_5.hdf5\n",
      "Treinamento 6 de 10, os pesos serão salvos em weights/architecture_4/training_6.hdf5\n",
      "Treinamento 7 de 10, os pesos serão salvos em weights/architecture_4/training_7.hdf5\n",
      "Treinamento 8 de 10, os pesos serão salvos em weights/architecture_4/training_8.hdf5\n",
      "Treinamento 9 de 10, os pesos serão salvos em weights/architecture_4/training_9.hdf5\n",
      "Treinamento 10 de 10, os pesos serão salvos em weights/architecture_4/training_10.hdf5\n"
     ]
    }
   ],
   "source": [
    "train_multiple_times(model_builder=model_2, times=10, weights_path_format='weights/architecture_4/training_{}.hdf5',\n",
    "                     x=X_train, y=y_train, batch_size=32, epochs=50, validation_data=(X_validation, y_validation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ry1iBJ21-RwO"
   },
   "source": [
    "### Arquitetura 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fZwcNUnN-RwR"
   },
   "outputs": [],
   "source": [
    "c_train  = load_images('Dataset_2/Train/C', filetype='tif', size=(128, 128))\n",
    "nc_train = load_images('Dataset_2/Train/NC', filetype='tif', size=(128, 128))\n",
    "\n",
    "c_validation  = load_images('Dataset_2/Validation/C', filetype='tif', size=(128, 128))\n",
    "nc_validation = load_images('Dataset_2/Validation/NC', filetype='tif', size=(128, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tusG_kwQ-Rwd"
   },
   "outputs": [],
   "source": [
    "X_train, y_train = prepare_data(c_train, nc_train, normalization_method=3, return_y=True)\n",
    "X_validation, y_validation = prepare_data(c_validation, nc_validation, normalization_method=3, return_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "TyXONvAp-Rwi",
    "outputId": "58203c66-1a92-48f1-cdf6-9e61fc34473b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treinamento 1 de 10, os pesos serão salvos em weights/architecture_5/training_1.hdf5\n",
      "Treinamento 2 de 10, os pesos serão salvos em weights/architecture_5/training_2.hdf5\n",
      "Treinamento 3 de 10, os pesos serão salvos em weights/architecture_5/training_3.hdf5\n",
      "Treinamento 4 de 10, os pesos serão salvos em weights/architecture_5/training_4.hdf5\n",
      "Treinamento 5 de 10, os pesos serão salvos em weights/architecture_5/training_5.hdf5\n",
      "Treinamento 6 de 10, os pesos serão salvos em weights/architecture_5/training_6.hdf5\n",
      "Treinamento 7 de 10, os pesos serão salvos em weights/architecture_5/training_7.hdf5\n",
      "Treinamento 8 de 10, os pesos serão salvos em weights/architecture_5/training_8.hdf5\n",
      "Treinamento 9 de 10, os pesos serão salvos em weights/architecture_5/training_9.hdf5\n",
      "Treinamento 10 de 10, os pesos serão salvos em weights/architecture_5/training_10.hdf5\n"
     ]
    }
   ],
   "source": [
    "train_multiple_times(model_builder=model_2, times=10, weights_path_format='weights/architecture_5/training_{}.hdf5',\n",
    "                     x=X_train, y=y_train, batch_size=32, epochs=50, validation_data=(X_validation, y_validation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JOyxnKQQP1Mu"
   },
   "source": [
    "### Arquitetura 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wk9U8Vn-P1M0"
   },
   "outputs": [],
   "source": [
    "c_train  = load_images('Dataset_2/Train/C', filetype='tif', size=(128, 128))\n",
    "nc_train = load_images('Dataset_2/Train/NC', filetype='tif', size=(128, 128))\n",
    "\n",
    "c_validation  = load_images('Dataset_2/Validation/C', filetype='tif', size=(128, 128))\n",
    "nc_validation = load_images('Dataset_2/Validation/NC', filetype='tif', size=(128, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oNAXAgYaP1M6"
   },
   "outputs": [],
   "source": [
    "X_train, y_train = prepare_data(c_train, nc_train, normalization_method=3, return_y=True)\n",
    "X_validation, y_validation = prepare_data(c_validation, nc_validation, normalization_method=3, return_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "OtoCv1d3P1M_",
    "outputId": "99ff2a20-4114-4f28-a42f-50f3c2198989"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treinamento 1 de 10, os pesos serão salvos em weights/architecture_6/training_1.hdf5\n",
      "Treinamento 2 de 10, os pesos serão salvos em weights/architecture_6/training_2.hdf5\n",
      "Treinamento 3 de 10, os pesos serão salvos em weights/architecture_6/training_3.hdf5\n",
      "Treinamento 4 de 10, os pesos serão salvos em weights/architecture_6/training_4.hdf5\n",
      "Treinamento 5 de 10, os pesos serão salvos em weights/architecture_6/training_5.hdf5\n",
      "Treinamento 6 de 10, os pesos serão salvos em weights/architecture_6/training_6.hdf5\n",
      "Treinamento 7 de 10, os pesos serão salvos em weights/architecture_6/training_7.hdf5\n",
      "Treinamento 8 de 10, os pesos serão salvos em weights/architecture_6/training_8.hdf5\n",
      "Treinamento 9 de 10, os pesos serão salvos em weights/architecture_6/training_9.hdf5\n",
      "Treinamento 10 de 10, os pesos serão salvos em weights/architecture_6/training_10.hdf5\n"
     ]
    }
   ],
   "source": [
    "train_multiple_times(model_builder=model_4, times=10, weights_path_format='weights/architecture_6/training_{}.hdf5',\n",
    "                     x=X_train, y=y_train, batch_size=32, epochs=50, validation_data=(X_validation, y_validation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1MC620tXeChM"
   },
   "source": [
    "### Arquitetura 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D6PeNpn2eChW"
   },
   "outputs": [],
   "source": [
    "c_train  = load_images('Dataset_2/Train/C', filetype='tif', size=(128, 128))\n",
    "nc_train = load_images('Dataset_2/Train/NC', filetype='tif', size=(128, 128))\n",
    "\n",
    "c_validation  = load_images('Dataset_2/Validation/C', filetype='tif', size=(128, 128))\n",
    "nc_validation = load_images('Dataset_2/Validation/NC', filetype='tif', size=(128, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qASkw8YFeChe"
   },
   "outputs": [],
   "source": [
    "X_train, y_train = prepare_data(c_train, nc_train, normalization_method=3, return_y=True)\n",
    "X_validation, y_validation = prepare_data(c_validation, nc_validation, normalization_method=3, return_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "8AeilFVseChh",
    "outputId": "2b4ab7ec-2bac-4194-b449-5e525fe42be4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treinamento 1 de 10, os pesos serão salvos em weights/architecture_7/training_1.hdf5\n",
      "Treinamento 2 de 10, os pesos serão salvos em weights/architecture_7/training_2.hdf5\n",
      "Treinamento 3 de 10, os pesos serão salvos em weights/architecture_7/training_3.hdf5\n",
      "Treinamento 4 de 10, os pesos serão salvos em weights/architecture_7/training_4.hdf5\n",
      "Treinamento 5 de 10, os pesos serão salvos em weights/architecture_7/training_5.hdf5\n",
      "Treinamento 6 de 10, os pesos serão salvos em weights/architecture_7/training_6.hdf5\n",
      "Treinamento 7 de 10, os pesos serão salvos em weights/architecture_7/training_7.hdf5\n",
      "Treinamento 8 de 10, os pesos serão salvos em weights/architecture_7/training_8.hdf5\n",
      "Treinamento 9 de 10, os pesos serão salvos em weights/architecture_7/training_9.hdf5\n",
      "Treinamento 10 de 10, os pesos serão salvos em weights/architecture_7/training_10.hdf5\n"
     ]
    }
   ],
   "source": [
    "train_multiple_times(model_builder=model_5, times=10, weights_path_format='weights/architecture_7/training_{}.hdf5',\n",
    "                     x=X_train, y=y_train, batch_size=32, epochs=50, validation_data=(X_validation, y_validation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uhmkRguQskmL"
   },
   "source": [
    "### Arquitetura 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JBfjlbvBskmV"
   },
   "outputs": [],
   "source": [
    "c_train  = load_images('Dataset_2/Train/C', filetype='tif', size=(128, 128))\n",
    "nc_train = load_images('Dataset_2/Train/NC', filetype='tif', size=(128, 128))\n",
    "\n",
    "c_validation  = load_images('Dataset_2/Validation/C', filetype='tif', size=(128, 128))\n",
    "nc_validation = load_images('Dataset_2/Validation/NC', filetype='tif', size=(128, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c_cUHTe-skml"
   },
   "outputs": [],
   "source": [
    "X_train, y_train = prepare_data(c_train, nc_train, normalization_method=3, return_y=True)\n",
    "X_validation, y_validation = prepare_data(c_validation, nc_validation, normalization_method=3, return_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "dUEY48Nvskmo",
    "outputId": "a815b075-a476-495b-a7d2-0a4a740a75b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treinamento 1 de 10, os pesos serão salvos em weights/architecture_8/training_1.hdf5\n",
      "Treinamento 2 de 10, os pesos serão salvos em weights/architecture_8/training_2.hdf5\n",
      "Treinamento 3 de 10, os pesos serão salvos em weights/architecture_8/training_3.hdf5\n",
      "Treinamento 4 de 10, os pesos serão salvos em weights/architecture_8/training_4.hdf5\n",
      "Treinamento 5 de 10, os pesos serão salvos em weights/architecture_8/training_5.hdf5\n",
      "Treinamento 6 de 10, os pesos serão salvos em weights/architecture_8/training_6.hdf5\n",
      "Treinamento 7 de 10, os pesos serão salvos em weights/architecture_8/training_7.hdf5\n",
      "Treinamento 8 de 10, os pesos serão salvos em weights/architecture_8/training_8.hdf5\n",
      "Treinamento 9 de 10, os pesos serão salvos em weights/architecture_8/training_9.hdf5\n",
      "Treinamento 10 de 10, os pesos serão salvos em weights/architecture_8/training_10.hdf5\n"
     ]
    }
   ],
   "source": [
    "train_multiple_times(model_builder=model_6, times=10, weights_path_format='weights/architecture_8/training_{}.hdf5',\n",
    "                     x=X_train, y=y_train, batch_size=32, epochs=50, validation_data=(X_validation, y_validation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "scwRNlXr6lmS"
   },
   "source": [
    "### Arquitetura 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kFINcueB6lmV"
   },
   "outputs": [],
   "source": [
    "c_train  = load_images('Dataset_2/Train/C', filetype='tif', size=(128, 128))\n",
    "nc_train = load_images('Dataset_2/Train/NC', filetype='tif', size=(128, 128))\n",
    "\n",
    "c_validation  = load_images('Dataset_2/Validation/C', filetype='tif', size=(128, 128))\n",
    "nc_validation = load_images('Dataset_2/Validation/NC', filetype='tif', size=(128, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vzK03jxc6lmb"
   },
   "outputs": [],
   "source": [
    "X_train, y_train = prepare_data(c_train, nc_train, normalization_method=3, return_y=True)\n",
    "X_validation, y_validation = prepare_data(c_validation, nc_validation, normalization_method=3, return_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "5b9yaIOI6lmf",
    "outputId": "a712be9c-bde5-468f-ab79-0a6f562c1c4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treinamento 1 de 10, os pesos serão salvos em weights/architecture_9/training_1.hdf5\n",
      "Treinamento 2 de 10, os pesos serão salvos em weights/architecture_9/training_2.hdf5\n",
      "Treinamento 3 de 10, os pesos serão salvos em weights/architecture_9/training_3.hdf5\n",
      "Treinamento 4 de 10, os pesos serão salvos em weights/architecture_9/training_4.hdf5\n",
      "Treinamento 5 de 10, os pesos serão salvos em weights/architecture_9/training_5.hdf5\n",
      "Treinamento 6 de 10, os pesos serão salvos em weights/architecture_9/training_6.hdf5\n",
      "Treinamento 7 de 10, os pesos serão salvos em weights/architecture_9/training_7.hdf5\n",
      "Treinamento 8 de 10, os pesos serão salvos em weights/architecture_9/training_8.hdf5\n",
      "Treinamento 9 de 10, os pesos serão salvos em weights/architecture_9/training_9.hdf5\n",
      "Treinamento 10 de 10, os pesos serão salvos em weights/architecture_9/training_10.hdf5\n"
     ]
    }
   ],
   "source": [
    "train_multiple_times(model_builder=model_7, times=10, weights_path_format='weights/architecture_9/training_{}.hdf5',\n",
    "                     x=X_train, y=y_train, batch_size=32, epochs=50, validation_data=(X_validation, y_validation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MInAsPAzyu6Z"
   },
   "source": [
    "### Arquitetura 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vOndIwQqyu6c"
   },
   "outputs": [],
   "source": [
    "c_train  = load_images('Dataset_2/Train/C', filetype='tif', size=(128, 128))\n",
    "nc_train = load_images('Dataset_2/Train/NC', filetype='tif', size=(128, 128))\n",
    "\n",
    "c_validation  = load_images('Dataset_2/Validation/C', filetype='tif', size=(128, 128))\n",
    "nc_validation = load_images('Dataset_2/Validation/NC', filetype='tif', size=(128, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xLiDgTAsyu6i"
   },
   "outputs": [],
   "source": [
    "X_train, y_train = prepare_data(c_train, nc_train, normalization_method=3, return_y=True)\n",
    "X_validation, y_validation = prepare_data(c_validation, nc_validation, normalization_method=3, return_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "50fcd30Qyu6o",
    "outputId": "ff479b79-89b1-4222-e8b5-edf4749a5597"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treinamento 1 de 10, os pesos serão salvos em weights/architecture_10/training_1.hdf5\n",
      "Treinamento 2 de 10, os pesos serão salvos em weights/architecture_10/training_2.hdf5\n",
      "Treinamento 3 de 10, os pesos serão salvos em weights/architecture_10/training_3.hdf5\n",
      "Treinamento 4 de 10, os pesos serão salvos em weights/architecture_10/training_4.hdf5\n",
      "Treinamento 5 de 10, os pesos serão salvos em weights/architecture_10/training_5.hdf5\n",
      "Treinamento 6 de 10, os pesos serão salvos em weights/architecture_10/training_6.hdf5\n",
      "Treinamento 7 de 10, os pesos serão salvos em weights/architecture_10/training_7.hdf5\n",
      "Treinamento 8 de 10, os pesos serão salvos em weights/architecture_10/training_8.hdf5\n",
      "Treinamento 9 de 10, os pesos serão salvos em weights/architecture_10/training_9.hdf5\n",
      "Treinamento 10 de 10, os pesos serão salvos em weights/architecture_10/training_10.hdf5\n"
     ]
    }
   ],
   "source": [
    "train_multiple_times(model_builder=model_8, times=10, weights_path_format='weights/architecture_10/training_{}.hdf5',\n",
    "                     x=X_train, y=y_train, batch_size=32, epochs=50, validation_data=(X_validation, y_validation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4ILWuMGhCczy"
   },
   "source": [
    "### Arquitetura 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eokxYW0_Ccz2"
   },
   "outputs": [],
   "source": [
    "c_train  = load_images('Dataset_2/Train/C', filetype='tif', size=(128, 128))\n",
    "nc_train = load_images('Dataset_2/Train/NC', filetype='tif', size=(128, 128))\n",
    "\n",
    "c_validation  = load_images('Dataset_2/Validation/C', filetype='tif', size=(128, 128))\n",
    "nc_validation = load_images('Dataset_2/Validation/NC', filetype='tif', size=(128, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RScxb_etCcz7"
   },
   "outputs": [],
   "source": [
    "X_train, y_train = prepare_data(c_train, nc_train, normalization_method=3, return_y=True)\n",
    "X_validation, y_validation = prepare_data(c_validation, nc_validation, normalization_method=3, return_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "5pKGCjGaCc0A",
    "outputId": "3f48025e-0364-44e2-958d-a98b48365769"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treinamento 1 de 10, os pesos serão salvos em weights/architecture_11/training_1.hdf5\n",
      "Treinamento 2 de 10, os pesos serão salvos em weights/architecture_11/training_2.hdf5\n",
      "Treinamento 3 de 10, os pesos serão salvos em weights/architecture_11/training_3.hdf5\n",
      "Treinamento 4 de 10, os pesos serão salvos em weights/architecture_11/training_4.hdf5\n",
      "Treinamento 5 de 10, os pesos serão salvos em weights/architecture_11/training_5.hdf5\n",
      "Treinamento 6 de 10, os pesos serão salvos em weights/architecture_11/training_6.hdf5\n",
      "Treinamento 7 de 10, os pesos serão salvos em weights/architecture_11/training_7.hdf5\n",
      "Treinamento 8 de 10, os pesos serão salvos em weights/architecture_11/training_8.hdf5\n",
      "Treinamento 9 de 10, os pesos serão salvos em weights/architecture_11/training_9.hdf5\n",
      "Treinamento 10 de 10, os pesos serão salvos em weights/architecture_11/training_10.hdf5\n"
     ]
    }
   ],
   "source": [
    "train_multiple_times(model_builder=model_9, times=10, weights_path_format='weights/architecture_11/training_{}.hdf5',\n",
    "                     x=X_train, y=y_train, batch_size=32, epochs=50, validation_data=(X_validation, y_validation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KjmKbG2FUR-O"
   },
   "source": [
    "### Arquitetura 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z2RLrzzTUR-S"
   },
   "outputs": [],
   "source": [
    "c_train  = load_images('Dataset_2/Train/C', filetype='tif', size=(128, 128))\n",
    "nc_train = load_images('Dataset_2/Train/NC', filetype='tif', size=(128, 128))\n",
    "\n",
    "c_validation  = load_images('Dataset_2/Validation/C', filetype='tif', size=(128, 128))\n",
    "nc_validation = load_images('Dataset_2/Validation/NC', filetype='tif', size=(128, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "10luZm08UR-X"
   },
   "outputs": [],
   "source": [
    "X_train, y_train = prepare_data(c_train, nc_train, normalization_method=3, return_y=True)\n",
    "X_validation, y_validation = prepare_data(c_validation, nc_validation, normalization_method=3, return_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "jZ8KwOw4UR-a",
    "outputId": "fa6bb2c6-a323-44a0-a4e2-cc72f0b79336"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treinamento 1 de 10, os pesos serão salvos em weights/architecture_12/training_1.hdf5\n",
      "Treinamento 2 de 10, os pesos serão salvos em weights/architecture_12/training_2.hdf5\n",
      "Treinamento 3 de 10, os pesos serão salvos em weights/architecture_12/training_3.hdf5\n",
      "Treinamento 4 de 10, os pesos serão salvos em weights/architecture_12/training_4.hdf5\n",
      "Treinamento 5 de 10, os pesos serão salvos em weights/architecture_12/training_5.hdf5\n",
      "Treinamento 6 de 10, os pesos serão salvos em weights/architecture_12/training_6.hdf5\n",
      "Treinamento 7 de 10, os pesos serão salvos em weights/architecture_12/training_7.hdf5\n",
      "Treinamento 8 de 10, os pesos serão salvos em weights/architecture_12/training_8.hdf5\n",
      "Treinamento 9 de 10, os pesos serão salvos em weights/architecture_12/training_9.hdf5\n",
      "Treinamento 10 de 10, os pesos serão salvos em weights/architecture_12/training_10.hdf5\n"
     ]
    }
   ],
   "source": [
    "train_multiple_times(model_builder=model_10, times=10, weights_path_format='weights/architecture_12/training_{}.hdf5',\n",
    "                     x=X_train, y=y_train, batch_size=32, epochs=50, validation_data=(X_validation, y_validation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k2oQ6q8ErJv2"
   },
   "source": [
    "### Arquitetura 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DyD5xEfQrJv6"
   },
   "outputs": [],
   "source": [
    "c_train  = load_images('Dataset_2/Train/C', filetype='tif', size=(128, 128))\n",
    "nc_train = load_images('Dataset_2/Train/NC', filetype='tif', size=(128, 128))\n",
    "\n",
    "c_validation  = load_images('Dataset_2/Validation/C', filetype='tif', size=(128, 128))\n",
    "nc_validation = load_images('Dataset_2/Validation/NC', filetype='tif', size=(128, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5UsuxuoUrJwA"
   },
   "outputs": [],
   "source": [
    "X_train, y_train = prepare_data(c_train, nc_train, normalization_method=3, return_y=True)\n",
    "X_validation, y_validation = prepare_data(c_validation, nc_validation, normalization_method=3, return_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "UUhdglkwrJwD",
    "outputId": "ea4d405e-8b67-46cc-e1fb-d9db7c09d732"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treinamento 1 de 10, os pesos serão salvos em weights/architecture_13/training_1.hdf5\n",
      "Treinamento 2 de 10, os pesos serão salvos em weights/architecture_13/training_2.hdf5\n",
      "Treinamento 3 de 10, os pesos serão salvos em weights/architecture_13/training_3.hdf5\n",
      "Treinamento 4 de 10, os pesos serão salvos em weights/architecture_13/training_4.hdf5\n",
      "Treinamento 5 de 10, os pesos serão salvos em weights/architecture_13/training_5.hdf5\n",
      "Treinamento 6 de 10, os pesos serão salvos em weights/architecture_13/training_6.hdf5\n",
      "Treinamento 7 de 10, os pesos serão salvos em weights/architecture_13/training_7.hdf5\n",
      "Treinamento 8 de 10, os pesos serão salvos em weights/architecture_13/training_8.hdf5\n",
      "Treinamento 9 de 10, os pesos serão salvos em weights/architecture_13/training_9.hdf5\n",
      "Treinamento 10 de 10, os pesos serão salvos em weights/architecture_13/training_10.hdf5\n"
     ]
    }
   ],
   "source": [
    "train_multiple_times(model_builder=model_11, times=10, weights_path_format='weights/architecture_13/training_{}.hdf5',\n",
    "                     x=X_train, y=y_train, batch_size=32, epochs=50, validation_data=(X_validation, y_validation))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "train_models.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
